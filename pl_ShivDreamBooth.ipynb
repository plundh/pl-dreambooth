{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plundh/pl-dreambooth/blob/main/pl_ShivDreamBooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "This notebook is forked from [this repository](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth) by Shivam Shriraro.\n",
        "\n",
        "*On your Google Drive:*\n",
        "1. Put your training images in ***dreambooth/training_images/[TRAINING_FOLDER_NAME]***\n",
        "2. Put your class images in ***dreambooth/class_images/[CLASS_FOLDER_NAME]***\n",
        "\n",
        "~ *NOTE: Needs a GPU with 15 GB VRAM or more.* ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU7NuMAA2drw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check Runtime GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TrKW_ncb-put"
      },
      "outputs": [],
      "source": [
        "#@title #1. Settings\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "GDRIVE_PATH = \"/content/google_drive\"\n",
        "if os.path.isdir(GDRIVE_PATH):\n",
        "  print(f\"Google Drive  already mounted at '{GDRIVE_PATH}'\" )\n",
        "else:\n",
        "  drive.mount(GDRIVE_PATH)\n",
        "\n",
        "#@markdown ###**Input Model**\n",
        "#@markdown This is the base model that will be modified for this training. Typically, this should be one of the standard Stable Diffusion models. \\\\\n",
        "#@markdown To download it automatically, you have to: \\\\\n",
        "#@markdown * Be a registered user in Hugging Face Hub and intput your [access token](https://huggingface.co/settings/tokens) here.\n",
        "#@markdown * Accept the model license before downloading or using the Stable Diffusion weights. Visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox to agree.  \n",
        "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "INPUT_MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param [\"runwayml/stable-diffusion-v1-5\", \"compvis/stable-diffusion-v1-4\"]\n",
        "\n",
        "#@markdown \\\n",
        "#@markdown ###**Training Images**\n",
        "#@markdown _/content/google_drive/MyDrive/dreambooth/training_images/[TRAINING_FOLDER_NAME]_ \\\\\n",
        "#@markdown These will be images of the concept you wish to train, ie. images of an object, art style or someone's likeness.\n",
        "#@markdown * Images: For objects and faces **10-30** training images are sufficient. For art styles, more are often better.\\\n",
        "#@markdown * You may train multiple concepts by adding additional dictionaries to **concepts_input** in the code. \\\\\n",
        "TRAINING_FOLDER_NAME = \"monet-images\" #@param {type:\"string\"}\n",
        "CONCEPT_NAME = \"p-picasso\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown \\\n",
        "#@markdown ###**Class Images**\n",
        "#@markdown _/content/google_drive/MyDrive/dreambooth/class_images/[CLASS_FOLDER_NAME]_ \\\\\n",
        "#@markdown '*Prior Preservation*' uses generated *Class Images* from the *Concept* class to prevent your trained concept from distoring other instances of that class. \\\n",
        "#@markdown * If no folder is provided, class images will be generated prior to training.\n",
        "#@markdown * The images should be generated by the Input Model.\n",
        "#@markdown ** **Example:** if your training images are of somebody's likeness, train using the class 'person'. The class images should then be images generated by the Input Model using the prompt 'person'.\n",
        "ENABLE_PRIOR_PRESERVATION = False #@param {type:\"boolean\"}\n",
        "CLASS_FOLDER_NAME = \"artstyle\" #@param {type:\"string\"}\n",
        "\n",
        "if ENABLE_PRIOR_PRESERVATION:\n",
        "  CLASS_NAME = \"painting\" #@param {type:\"string\"}\n",
        "  class_name_space = \" \" + CLASS_NAME\n",
        "else:\n",
        "  CLASS_NAME = \"\"\n",
        "  class_name_space = CLASS_NAME\n",
        "\n",
        "TRAINING_IMAGES_ROOT = GDRIVE_PATH + \"/MyDrive/dreambooth/training_images/\"\n",
        "CLASS_IMAGES_ROOT = GDRIVE_PATH + \"/MyDrive/dreambooth/class_images/\"\n",
        "\n",
        "#@markdown \\\n",
        "#@markdown ###**Training Settings**\n",
        "#@markdown *'Steps'* is the unit describing how long and thorough the model will train. \\\n",
        "#@markdown * Too few steps, and the model will lack the ability to replicate desired characteristists from the *Training Images*. Too long, and it will overfit, meaning it will generate images that have characterists more closely resembling the training data than the prompt.\n",
        "#@markdown * The appropriate number of steps scales more or less linearly with the number of *Training Images*, so *Steps Per Image* is used as a convenient metric. \\\n",
        "#@markdown * **60-100** *Steps Per Image* is a good starting point, but there is no magic formula. \\\n",
        "#@markdown \\\n",
        "#@markdown Checkpoints will be saved as '**[date]** \\_( **[token]** @ **[training_image_folder]** \\_ **[image_count]** i)_ **[total images]** _ **{steps per total images}** .ckpt' \\\\\n",
        "#@markdown To facillitate finding the optimal number of *Steps*, checkpoints can be saved at *Steps Per Image* intervals.\n",
        "\n",
        "STEPS_PER_IMAGE = 100 #@param {type:\"integer\"}\n",
        "STEPS_PER_IMAGE_SAVE_INTERVAL = 20 #@param {type:\"integer\"}\n",
        "\n",
        "### CONCEPTS GO HERE #################################################################\n",
        "# Add a new dictionary to 'concepts_input' for each extra concept you wish to train.\n",
        "\n",
        "concepts_input = [\n",
        "    {\n",
        "        \"CONCEPT_NAME\":                   CONCEPT_NAME,\n",
        "        \"class_name\":                     CLASS_NAME,\n",
        "        \"TRAINING_FOLDER_NAME\":           TRAINING_FOLDER_NAME,\n",
        "        \"CLASS_FOLDER_NAME\":              CLASS_FOLDER_NAME\n",
        "#    },\n",
        "#    {\n",
        "#        \"CONCEPT_NAME\":                  \"c-monet\",\n",
        "#        \"class_name\":                    \"artstyle\",\n",
        "#        \"TRAINING_FOLDER_NAME\":          \"monet-images\",\n",
        "#        \"CLASS_FOLDER_NAME\":             \"artstyle_ddim\"\n",
        "    }\n",
        "]\n",
        "######################################################################################\n",
        "EXPORT_ONLY_LAST_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown \\\n",
        "#@markdown ###**Sample Generation**\n",
        "#@markdown Image grids of different *'prompts'*, *'steps'* and *'CFG'* settings can be generated for each checkpoint in order to stress test the model.  \\\n",
        "#@markdown * **Note:** This step adds considerable time.\n",
        "ENABLE_SAMPLE_GENERATION = True #@param {type:\"boolean\"}\n",
        "\n",
        "SAMPLE_PROMPT_1 = \"portrait film still of jaime lannister, patterned cape, woolen shirt, cinematic\" #@param {type:\"string\"}\n",
        "SAMPLE_PROMPT_2 = \"alhambra exterior courtyard, garden\" #@param {type:\"string\"}\n",
        "SAMPLE_PROMPT_3 = \"close-up of lion on savannah, three-quarter view, photo\" #@param {type:\"string\"}\n",
        "SAMPLE_PROMPT_4 = \"spaceship cockpit, interior, sci-fi, photo\" #@param {type:\"string\"}\n",
        "\n",
        "SAMPLE_PROMPT_LIST = [\n",
        "    f\"{CONCEPT_NAME}{class_name_space}, {SAMPLE_PROMPT_1}\",\n",
        "    f\"{CONCEPT_NAME}{class_name_space}, {SAMPLE_PROMPT_2}\",\n",
        "    f\"{CONCEPT_NAME}{class_name_space}, {SAMPLE_PROMPT_3}\",\n",
        "    f\"{CONCEPT_NAME}{class_name_space}, {SAMPLE_PROMPT_4}\",\n",
        "    f\"{CONCEPT_NAME}{class_name_space}\"\n",
        "    ]\n",
        "\n",
        "#@markdown \\\n",
        "DISCONNECT_ON_COMPLETION = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R5GeU5_MZ77z"
      },
      "outputs": [],
      "source": [
        "#@title #2. Verify Settings and Training Data\n",
        "\n",
        "import math\n",
        "import fnmatch\n",
        "import json\n",
        "from collections import Counter\n",
        "from prettytable import PrettyTable\n",
        "from prettytable import SINGLE_BORDER\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "concepts_list = []\n",
        "checkpoint_list = []\n",
        "\n",
        "MODELS_ROOT = f\"{GDRIVE_PATH}/MyDrive/dreambooth/models\"\n",
        "\n",
        "for concept in concepts_input:\n",
        "  instance_values = {\n",
        "        \"instance_prompt\":      concept[\"CONCEPT_NAME\"],\n",
        "        \"instance_data_dir\":    f\"{TRAINING_IMAGES_ROOT}{concept['TRAINING_FOLDER_NAME']}/\",\n",
        "        \"inst_file_count\":      len(os.listdir(f\"{TRAINING_IMAGES_ROOT}{concept['TRAINING_FOLDER_NAME']}\")),\n",
        "     }\n",
        "  \n",
        "  if ENABLE_PRIOR_PRESERVATION:\n",
        "    class_values ={\n",
        "      \"class_prompt\":         concept[\"class_name\"],\n",
        "      \"class_data_dir\":       f\"{CLASS_IMAGES_ROOT}{concept['CLASS_FOLDER_NAME']}/\",\n",
        "      \"class_file_count\":     len(os.listdir(f\"{CLASS_IMAGES_ROOT}{concept['CLASS_FOLDER_NAME']}\"))\n",
        "    }\n",
        "    combined_values = {**instance_values, **class_values}\n",
        "    concepts_list.append(combined_values)\n",
        "  else:\n",
        "    concepts_list.append(instance_values)\n",
        "\n",
        "if not ENABLE_PRIOR_PRESERVATION: print(\"☑️ Skipping Prior Preservation\")\n",
        "\n",
        "#print(json.dumps(concepts_list, sort_keys=True, indent=2)) # For debugging\n",
        "#print(\"\\n\")\n",
        "\n",
        "if len(concepts_list) == 1:\n",
        "    print(\"Training 1 concept\")\n",
        "else:\n",
        "    print(f\"Training {len(concepts_list)} concepts\")\n",
        "\n",
        "for concept in concepts_list:\n",
        "  print(\"\\n\")\n",
        "  class_name = f\"_{concept['class_prompt']}\" if ENABLE_PRIOR_PRESERVATION else ''\n",
        "  combined_token = \",\".join([concept['instance_prompt']for concept in concepts_list])\n",
        "  combined_token_class_folder = \",\".join([\"(\" + str(concept['instance_prompt']) + class_name + \"@\" + str(os.path.basename(os.path.normpath(concept['instance_data_dir']))) + \"_\" + str(concept['inst_file_count']) + \"i\" + \")\" for concept in concepts_list])\n",
        "  if concept['inst_file_count'] == 0:\n",
        "    print(f\"❌ No training images found in '{concept['instance_data_dir']}'\")\n",
        "  else:\n",
        "    print(f\"✅ {concept['inst_file_count']} training images found in '{concept['instance_data_dir']}'\")\n",
        "\n",
        "  if ENABLE_PRIOR_PRESERVATION:\n",
        "    if concept['class_file_count'] == 0:\n",
        "          print(f\"❌ No Class images found in '{concept['class_data_dir']}'\")\n",
        "    else:\n",
        "      print(f\"✅ {concept['class_file_count']} class images found in '{concept['class_data_dir']}'\")\n",
        "\n",
        "  concept_table = PrettyTable(header=False)\n",
        "  concept_table.align = \"l\"\n",
        "  concept_table.set_style(SINGLE_BORDER)\n",
        "\n",
        "  for key, value in concept.items():\n",
        "    concept_table.add_row([key, value])\n",
        "  print(concept_table)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)\n",
        "\n",
        "total_class_images = sum([concept['inst_file_count'] for concept in concepts_list])\n",
        "\n",
        "## Output paths\n",
        "total_training_images = sum([concept[\"inst_file_count\"] for concept in concepts_list])\n",
        "date_string = !date +\"%Y-%m-%d_%H-%M\"\n",
        "training_steps = total_training_images * STEPS_PER_IMAGE\n",
        "save_interval = total_training_images * STEPS_PER_IMAGE_SAVE_INTERVAL\n",
        "\n",
        "temp_folder = f\"{date_string[-1]}_{combined_token_class_folder}\"\n",
        "temp_folder = f\"stable_diffusion_weights/{temp_folder}\"\n",
        "\n",
        "os.makedirs(temp_folder, exist_ok=True)\n",
        "os.makedirs(MODELS_ROOT, exist_ok=True)\n",
        "os.makedirs(f\"{GDRIVE_PATH}/MyDrive/dreambooth/training_images\", exist_ok=True)\n",
        "os.makedirs(f\"{GDRIVE_PATH}/MyDrive/dreambooth/class_images\", exist_ok=True)\n",
        "\n",
        "checkpoint_steps = training_steps\n",
        "checkpoints_table = PrettyTable(['Step', 'Steps per image'])\n",
        "checkpoints_table.set_style(SINGLE_BORDER)\n",
        "checkpoints_table.sortby = \"Step\"\n",
        "\n",
        "\n",
        "def output_file_name(steps_per_img):\n",
        "  return f\"{date_string[-1]}_{combined_token_class_folder}_[{total_training_images}]_{{{steps_per_img}}}\"\n",
        "\n",
        "model_output_dir = f\"{MODELS_ROOT}/{output_file_name(STEPS_PER_IMAGE)}\"\n",
        "os.makedirs(model_output_dir, exist_ok=True)\n",
        "\n",
        "def output_file_path(steps_per_img):\n",
        "  return f\"{model_output_dir}/{output_file_name(steps_per_img)}.ckpt\"\n",
        "\n",
        "while checkpoint_steps > 0:\n",
        "  steps_per_img = int(checkpoint_steps / total_training_images)\n",
        "  checkpoints_table.add_row([checkpoint_steps, steps_per_img])\n",
        "  checkpoint_list.append({\"steps\": checkpoint_steps, \"steps_per_img\": steps_per_img, \"output_file_name\": output_file_name(steps_per_img), \"output_file_path\": output_file_path(steps_per_img), \"temp_folder_path\": f\"{temp_folder}/{checkpoint_steps}\"})\n",
        "  checkpoint_steps -= STEPS_PER_IMAGE_SAVE_INTERVAL * total_training_images\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Saving checkpoints every {STEPS_PER_IMAGE_SAVE_INTERVAL} steps per image:\")\n",
        "print(checkpoints_table)\n",
        "print(\"\\n\")\n",
        "print(\"Ready for training ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n6dcjPnnaiCn"
      },
      "outputs": [],
      "source": [
        "#@title #3. Install Requirements\n",
        "\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio natsort\n",
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers\n",
        "\n",
        "!mkdir -p ~/.huggingface\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "#@title #4. Training\n",
        "\n",
        "if ENABLE_PRIOR_PRESERVATION:\n",
        "  !accelerate launch train_dreambooth.py \\\n",
        "    --pretrained_model_name_or_path=\"{INPUT_MODEL_NAME}\" \\\n",
        "    --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "    --output_dir=\"{temp_folder}\" \\\n",
        "    --with_prior_preservation \\\n",
        "    --prior_loss_weight=1.0 \\\n",
        "    --num_class_images=1000  \\\n",
        "    --seed=1337 \\\n",
        "    --resolution=512 \\\n",
        "    --train_batch_size=1 \\\n",
        "    --train_text_encoder \\\n",
        "    --revision=\"fp16\" \\\n",
        "    --use_8bit_adam \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --learning_rate=1e-6 \\\n",
        "    --lr_scheduler=\"constant\" \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --max_train_steps=\"{training_steps}\" \\\n",
        "    --save_interval=\"{save_interval}\" \\\n",
        "    --pad_tokens \\\n",
        "    --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "else:\n",
        "  !accelerate launch train_dreambooth.py \\\n",
        "    --pretrained_model_name_or_path=\"{INPUT_MODEL_NAME}\" \\\n",
        "    --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "    --output_dir=\"{temp_folder}\" \\\n",
        "    --seed=1337 \\\n",
        "    --resolution=512 \\\n",
        "    --train_batch_size=1 \\\n",
        "    --train_text_encoder \\\n",
        "    --revision=\"fp16\" \\\n",
        "    --use_8bit_adam \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --learning_rate=5e-6 \\\n",
        "    --lr_scheduler=\"constant\" \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --max_train_steps=\"{training_steps}\" \\\n",
        "    --save_interval=\"{save_interval}\" \\\n",
        "    --pad_tokens \\\n",
        "    --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@title #5. Export Checkpoint files to Google Drive\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "half_arg = \"\"\n",
        "fp16 = True\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "\n",
        "def convert_diffusers(checkpoint_folder_name, steps_per_img):\n",
        "  try:\n",
        "    !python convert_diffusers_to_original_stable_diffusion.py --model_path \"{checkpoint_folder_name}\" --checkpoint_path \"{output_file_path(steps_per_img)}\" --half\n",
        "    print(f\"✅ Saved model '{output_file_path(steps_per_img)}'\")\n",
        "  except:\n",
        "    print(f\"❌ Could not save model '{output_file_name(training_steps)}.ckpt'\")\n",
        "\n",
        "if EXPORT_ONLY_LAST_CHECKPOINT:\n",
        "  convert_diffusers(checkpoint_list[0][\"temp_folder_path\"], checkpoint_list[0][\"steps_per_img\"])\n",
        "else:\n",
        "  for checkpoint in checkpoint_list:\n",
        "    convert_diffusers(checkpoint[\"temp_folder_path\"], checkpoint[\"steps_per_img\"])\n",
        "\n",
        "if ENABLE_PRIOR_PRESERVATION:\n",
        "  print(f\"To invoke your trained subject, use '{combined_token_class_folder}' in the prompt.\")\n",
        "else:\n",
        "  print(f\"To invoke your trained subject, use '{combined_token}' in the prompt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPEpRCBxaHsj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #5. Generate Sample Images\n",
        "import torch\n",
        "from contextlib import closing\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "from google.colab import runtime\n",
        "from tqdm import tqdm\n",
        "import numpy\n",
        "!wget https://github.com/MaxGhenis/random/raw/master/Roboto-Regular.ttf\n",
        "\n",
        "numpy.random.seed(10)\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\\\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "if ENABLE_SAMPLE_GENERATION:\n",
        "  os.makedirs(f'{model_output_dir}/samples', exist_ok=True)\n",
        "  sample_steps = [10, 30, 60, 100, 150]\n",
        "  sample_CFG = [4, 6, 9, 13, 18]\n",
        "  height = 512\n",
        "  width = 512\n",
        "\n",
        "  print(f\"Generating {len(checkpoint_list)} sample grids with {len(sample_steps) * len(sample_CFG)} images of size {width}px * {height}px ...\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  for checkpoint in tqdm(checkpoint_list):\n",
        "    checkpoint[\"sample_images\"] = []\n",
        "    device = \"cuda\"\n",
        "    scheduler = DDIMScheduler(\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        clip_sample=False,\n",
        "        set_alpha_to_one=False\n",
        "        )\n",
        "    \n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        checkpoint['temp_folder_path'],\n",
        "        scheduler=scheduler,\n",
        "        revision=\"fp16\",\n",
        "        torch_dtype=torch.float16\n",
        "        ).to(device)\n",
        "\n",
        "    generator = torch.Generator(device=device)\n",
        "\n",
        "    latents = None\n",
        "    seeds = []\n",
        "    seed = generator.seed()\n",
        "    seeds.append(seed)\n",
        "    generator = generator.manual_seed(10)\n",
        "    \n",
        "    image_latents = torch.randn(\n",
        "        (1, pipe.unet.in_channels, height // 8, width // 8),\n",
        "        generator=generator,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    latents = image_latents    \n",
        "    # latents should have shape (4, 4, 64, 64) in this case\n",
        "    # print(latents.shape)\n",
        "\n",
        "    for prompt in SAMPLE_PROMPT_LIST:\n",
        "      images = []\n",
        "      print(\"\\n\")\n",
        "      print(f\"Generating prompt '{prompt}' ...\")\n",
        "      \n",
        "      for CFG in sample_CFG:\n",
        "        for steps in sample_steps:\n",
        "          with autocast(\"cuda\"):\n",
        "            image = pipe(\n",
        "                prompt,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=CFG,\n",
        "                latents=latents,\n",
        "                ).images\n",
        "\n",
        "            im = ImageDraw.Draw(image[-1])\n",
        "            myFont = ImageFont.truetype('Roboto-Regular.ttf', 20)\n",
        "            im.multiline_text((6, 3), f\"Steps: {steps}\\nCFG: {CFG}\", font=myFont, fill=(255, 255, 255))\n",
        "            images = images + image\n",
        "\n",
        "      grid = image_grid(images, rows=len(sample_CFG), cols=len(sample_steps))\n",
        "      im = ImageDraw.Draw(grid)\n",
        "      myFont = ImageFont.truetype('Roboto-Regular.ttf', 30)\n",
        "      im.multiline_text((6, 50), f\"Prompt: '{prompt}'\\nSteps Per Image: {checkpoint['steps_per_img']}\", font=myFont, fill=(255, 255, 255))\n",
        "      \n",
        "      file_path = f'{model_output_dir}/samples/<{prompt}>{checkpoint[\"output_file_name\"]}.jpg'\n",
        "      checkpoint[\"sample_images\"].append({\"img\": grid, \"img_path\": file_path})\n",
        "  \n",
        "  # Debug\n",
        "  #import pprint\n",
        "  #pp = pprint.PrettyPrinter(indent=4)\n",
        "  #pp.pprint(checkpoint_list)\n",
        "\n",
        "  for checkpoint in checkpoint_list:\n",
        "    for image in checkpoint[\"sample_images\"]:\n",
        "      image[\"img\"].save(image[\"img_path\"])\n",
        "      print(f'Saved {image[\"img_path\"]}')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"✅ Done!\")\n",
        "\n",
        "if DISCONNECT_ON_COMPLETION:\n",
        "  print(\"Disconnecting Runtime ...\")\n",
        "  runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}